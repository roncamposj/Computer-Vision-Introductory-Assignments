{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsFuVzKsCqXJhvX0QlEfHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roncamposj/Computer-Vision-Introductory-Assignments/blob/main/Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1"
      ],
      "metadata": {
        "id": "AVpx3W69i6_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zCzxvq6gmoe"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#torch.ones is an input and torchrand(requires_grad) = a parameter, you update it.\n",
        "\n",
        "#loss = -y*torch.log(p)-(1-y)*torch.log(1-p)\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, mode):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        # Define various layers here\n",
        "        self.flatten = nn.Flatten()\n",
        "        #for model 1 and 2 use\n",
        "        self.fc1 = nn.Linear(28*28, 20)\n",
        "        self.fc2 = nn.Linear(20, 20)\n",
        "        self.fc3 = nn.Linear(20, 10)\n",
        "        \n",
        "        #for model 3 use\n",
        "        self.alt1 = nn.Linear(28*28, 202)\n",
        "        self.alt2 = nn.Linear(202, 202)\n",
        "        self.alt3 = nn.Linear(202, 202)\n",
        "        self.alt4 = nn.Linear(202, 202)\n",
        "        self.alt5 = nn.Linear(202, 10)\n",
        "        \n",
        "        # This will select the forward pass function based on mode for the ConvNet.\n",
        "        # During creation of each ConvNet model, you will assign one of the valid mode.\n",
        "        # This will fix the forward function (and the network graph) for the entire training/testing\n",
        "        if mode == 1:\n",
        "            self.forward = self.model_1\n",
        "        elif mode == 2:\n",
        "            self.forward = self.model_2\n",
        "        elif mode == 3:\n",
        "            self.forward = self.model_3\n",
        "        else: \n",
        "            print(\"Invalid mode \", mode, \"selected. Select between 1-3\")\n",
        "            exit(0)\n",
        "      \n",
        "    # Baseline sample model\n",
        "    def model_0(self, X):\n",
        "        # ======================================================================\n",
        "        # Three fully connected layers with activation\n",
        "        \n",
        "        X = torch.flatten(X, start_dim=1)\n",
        "        X = self.fc1(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.fc2(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.fc3(X)\n",
        "        X = torch.sigmoid(X)\n",
        "                \n",
        "        return X  \n",
        "        \n",
        "    # Baseline model. task 1\n",
        "    def model_1(self, X):\n",
        "        # ======================================================================\n",
        "        # Three fully connected layers without activation\n",
        "\n",
        "\n",
        "        X = self.flatten(X)\n",
        "        # NotImplementedError\n",
        "        X = self.fc1(X)\n",
        "        X = self.fc2(X)\n",
        "        X = self.fc3(X)\n",
        "                        \n",
        "        return X\n",
        "        \n",
        "\n",
        "    # task 2\n",
        "    def model_2(self, X):\n",
        "        # ======================================================================\n",
        "        # Train with activation (use model 1 from task 1)\n",
        "\n",
        "        X = self.flatten(X)\n",
        "        X = self.fc1(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.fc2(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.fc3(X)\n",
        "        X = torch.sigmoid(X)\n",
        "        \n",
        "        return X\n",
        "\n",
        "\t\n",
        "    # task 3\n",
        "    def model_3(self, X):\n",
        "        # ======================================================================\n",
        "        # Change number of fully connected layers and number of neurons from model 2 in task 2\n",
        "\n",
        "        X = self.flatten(X)\n",
        "        X = self.alt1(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.alt2(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.alt3(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.alt4(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.alt5(X)\n",
        "        X = torch.sigmoid(X)\n",
        "\n",
        "        \n",
        "        return X       \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# from ConvNet import ConvNet \n",
        "import argparse\n",
        "import numpy as np \n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch, batch_size):\n",
        "    '''\n",
        "    Trains the model for an epoch and optimizes it.\n",
        "    model: The model to train. Should already be in correct device.\n",
        "    device: 'cuda' or 'cpu'.\n",
        "    train_loader: dataloader for training samples.\n",
        "    optimizer: optimizer to use for model parameter updates.\n",
        "    criterion: used to compute loss for prediction and target \n",
        "    epoch: Current epoch to train for.\n",
        "    batch_size: Batch size to be used.\n",
        "    '''\n",
        "    \n",
        "    # Set model to train mode before each epoch\n",
        "    model.train()\n",
        "    \n",
        "    # Empty list to store losses \n",
        "    losses = []\n",
        "    correct = 0\n",
        "    \n",
        "    # Iterate over entire training samples (1 epoch)\n",
        "    for batch_idx, batch_sample in enumerate(train_loader):\n",
        "        data, target = batch_sample\n",
        "        \n",
        "        # Push data/label to correct device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Do forward pass for current set of data\n",
        "        output = model(data)\n",
        "        \n",
        "        # ======================================================================\n",
        "        # Compute loss based on criterion\n",
        "        # ----------------- YOUR CODE HERE ----------------------\n",
        "        #\n",
        "        # Remove NotImplementedError and assign correct loss function.\n",
        "\n",
        "        loss = criterion(output,target)\n",
        "        #F.nll_loss(output, target)\n",
        "\n",
        "        #nn.MSELoss() is an alternative for above.\n",
        "        \n",
        "        \n",
        "        # Computes gradient based on final loss\n",
        "        loss.backward()\n",
        "        \n",
        "        # Store loss\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        # Optimize model parameters based on learning rate and gradient \n",
        "        optimizer.step()\n",
        "        \n",
        "        # Get predicted index by selecting maximum log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        \n",
        "        # ======================================================================\n",
        "        # Count correct predictions overall \n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        \n",
        "    train_loss = float(np.mean(losses))\n",
        "    train_acc = correct / ((batch_idx+1) * batch_size)\n",
        "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n",
        "        100. * correct / ((batch_idx+1) * batch_size)))\n",
        "    return train_loss, train_acc\n",
        "    \n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    '''\n",
        "    Tests the model.\n",
        "    model: The model to train. Should already be in correct device.\n",
        "    device: 'cuda' or 'cpu'.\n",
        "    test_loader: dataloader for test samples.\n",
        "    '''\n",
        "    \n",
        "    # Set model to eval mode to notify all layers.\n",
        "    model.eval()\n",
        "    \n",
        "    losses = []\n",
        "    correct = 0\n",
        "    \n",
        "    # Set torch.no_grad() to disable gradient computation and backpropagation\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, sample in enumerate(test_loader):\n",
        "            data, target = sample\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "\n",
        "            # Predict for data by doing forward pass\n",
        "            output = model(data)\n",
        "            \n",
        "            # ======================================================================\n",
        "            # Compute loss based on same criterion as training\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(output,target)\n",
        "            \n",
        "            # Append loss to overall test loss\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            # Get predicted index by selecting maximum log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            \n",
        "            # ======================================================================\n",
        "            # Count correct predictions overall \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss = float(np.mean(losses))\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "    \n",
        "    return test_loss, accuracy\n",
        "    \n",
        "\n",
        "def run_main(FLAGS):\n",
        "    # Check if cuda is available\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    \n",
        "    # Set proper device based on cuda availability \n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print(\"Torch device selected: \", device)\n",
        "    \n",
        "    # Initialize the model and send to device \n",
        "    model = ConvNet(FLAGS.mode).to(device)\n",
        "    \n",
        "    # Initialize the criterion for loss computation \n",
        "    # ======================================================================\n",
        "    # Remove NotImplementedError and assign correct loss function.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    \n",
        "    \n",
        "    # Initialize optimizer type \n",
        "    optimizer = optim.SGD(model.parameters(), lr=FLAGS.learning_rate, weight_decay=1e-7)\n",
        "    \n",
        "    # Create transformations to apply to each data sample \n",
        "    # Can specify variations such as image flip, color flip, random crop, ...\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    \n",
        "    # Load datasets for training and testing\n",
        "    # Inbuilt datasets available in torchvision (check documentation online)\n",
        "    dataset1 = datasets.MNIST('./data/', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset2 = datasets.MNIST('./data/', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = DataLoader(dataset1, batch_size = FLAGS.batch_size, \n",
        "                                shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(dataset2, batch_size = FLAGS.batch_size, \n",
        "                                shuffle=False, num_workers=4)\n",
        "    \n",
        "    best_accuracy = 0.0\n",
        "    \n",
        "    # Run training for n_epochs specified in config \n",
        "    for epoch in range(1, FLAGS.num_epochs + 1):\n",
        "        print(\"\\nEpoch: \", epoch)\n",
        "        train_loss, train_accuracy = train(model, device, train_loader,\n",
        "                                            optimizer, criterion, epoch, FLAGS.batch_size)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        \n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            \n",
        "    print(\"accuracy is {:2.2f}\".format(best_accuracy))\n",
        "    \n",
        "    print(\"Training and evaluation finished\")\n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    # Set parameters for Sparse Autoencoder\n",
        "    parser = argparse.ArgumentParser('CNN Exercise.')\n",
        "    parser.add_argument('--mode',\n",
        "                        type=int, default=1,\n",
        "                        help='Select mode between 1-3.')\n",
        "    parser.add_argument('--learning_rate',\n",
        "                        type=float, default=0.1,\n",
        "                        help='Initial learning rate.')\n",
        "    parser.add_argument('--num_epochs',\n",
        "                        type=int,\n",
        "                        default=20,\n",
        "                        help='Number of epochs to run trainer.')\n",
        "    parser.add_argument('--batch_size',\n",
        "                        type=int, default=10,\n",
        "                        help='Batch size. Must divide evenly into the dataset sizes.')\n",
        "    parser.add_argument('--log_dir',\n",
        "                        type=str,\n",
        "                        default='logs',\n",
        "                        help='Directory to put logging.')\n",
        "                        \n",
        "    FLAGS = None\n",
        "    FLAGS, unparsed = parser.parse_known_args()\n",
        "    \n",
        "    print(\"Mode: \", FLAGS.mode)\n",
        "    print(\"LR: \", FLAGS.learning_rate)\n",
        "    print(\"Batch size: \", FLAGS.batch_size)\n",
        "    \n",
        "    run_main(FLAGS)\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhhx6koHi0e4",
        "outputId": "45789607-5612-498a-b350-7f54c25b1b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode:  3\n",
            "LR:  0.1\n",
            "Batch size:  10\n",
            "Torch device selected:  cpu\n",
            "\n",
            "Epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: Average loss: 1.6040, Accuracy: 48601/60000 (81%)\n",
            "Test set: Average loss: 1.5119, Accuracy: 9372/10000 (94%)\n",
            "\n",
            "Epoch:  2\n",
            "Train set: Average loss: 1.5019, Accuracy: 56960/60000 (95%)\n",
            "Test set: Average loss: 1.4972, Accuracy: 9570/10000 (96%)\n",
            "\n",
            "Epoch:  3\n",
            "Train set: Average loss: 1.4906, Accuracy: 57762/60000 (96%)\n",
            "Test set: Average loss: 1.5011, Accuracy: 9461/10000 (95%)\n",
            "\n",
            "Epoch:  4\n",
            "Train set: Average loss: 1.4851, Accuracy: 58150/60000 (97%)\n",
            "Test set: Average loss: 1.4877, Accuracy: 9655/10000 (97%)\n",
            "\n",
            "Epoch:  5\n",
            "Train set: Average loss: 1.4825, Accuracy: 58396/60000 (97%)\n",
            "Test set: Average loss: 1.4817, Accuracy: 9724/10000 (97%)\n",
            "\n",
            "Epoch:  6\n",
            "Train set: Average loss: 1.4805, Accuracy: 58492/60000 (97%)\n",
            "Test set: Average loss: 1.4825, Accuracy: 9717/10000 (97%)\n",
            "\n",
            "Epoch:  7\n",
            "Train set: Average loss: 1.4788, Accuracy: 58655/60000 (98%)\n",
            "Test set: Average loss: 1.4835, Accuracy: 9720/10000 (97%)\n",
            "\n",
            "Epoch:  8\n",
            "Train set: Average loss: 1.4777, Accuracy: 58732/60000 (98%)\n",
            "Test set: Average loss: 1.4833, Accuracy: 9709/10000 (97%)\n",
            "\n",
            "Epoch:  9\n",
            "Train set: Average loss: 1.4758, Accuracy: 58874/60000 (98%)\n",
            "Test set: Average loss: 1.4825, Accuracy: 9729/10000 (97%)\n",
            "\n",
            "Epoch:  10\n",
            "Train set: Average loss: 1.4748, Accuracy: 58916/60000 (98%)\n",
            "Test set: Average loss: 1.4865, Accuracy: 9702/10000 (97%)\n",
            "\n",
            "Epoch:  11\n",
            "Train set: Average loss: 1.4742, Accuracy: 59010/60000 (98%)\n",
            "Test set: Average loss: 1.4819, Accuracy: 9747/10000 (97%)\n",
            "\n",
            "Epoch:  12\n",
            "Train set: Average loss: 1.4745, Accuracy: 58987/60000 (98%)\n",
            "Test set: Average loss: 1.4853, Accuracy: 9672/10000 (97%)\n",
            "\n",
            "Epoch:  13\n",
            "Train set: Average loss: 1.4739, Accuracy: 58985/60000 (98%)\n",
            "Test set: Average loss: 1.4806, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Epoch:  14\n",
            "Train set: Average loss: 1.4732, Accuracy: 58997/60000 (98%)\n",
            "Test set: Average loss: 1.4890, Accuracy: 9653/10000 (97%)\n",
            "\n",
            "Epoch:  15\n",
            "Train set: Average loss: 1.4729, Accuracy: 59105/60000 (99%)\n",
            "Test set: Average loss: 1.4798, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "Epoch:  16\n",
            "Train set: Average loss: 1.4728, Accuracy: 59067/60000 (98%)\n",
            "Test set: Average loss: 1.4840, Accuracy: 9688/10000 (97%)\n",
            "\n",
            "Epoch:  17\n",
            "Train set: Average loss: 1.4721, Accuracy: 59147/60000 (99%)\n",
            "Test set: Average loss: 1.4814, Accuracy: 9740/10000 (97%)\n",
            "\n",
            "Epoch:  18\n",
            "Train set: Average loss: 1.4732, Accuracy: 59034/60000 (98%)\n",
            "Test set: Average loss: 1.4828, Accuracy: 9716/10000 (97%)\n",
            "\n",
            "Epoch:  19\n",
            "Train set: Average loss: 1.4722, Accuracy: 59150/60000 (99%)\n",
            "Test set: Average loss: 1.4830, Accuracy: 9731/10000 (97%)\n",
            "\n",
            "Epoch:  20\n",
            "Train set: Average loss: 1.4722, Accuracy: 59099/60000 (98%)\n",
            "Test set: Average loss: 1.4876, Accuracy: 9645/10000 (96%)\n",
            "accuracy is 97.65\n",
            "Training and evaluation finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1:**\n",
        "\n",
        "The final accuracy was 78.93, which can only be attributed to epoch 1.  The final training and testing accuracies at epoch 20 were abysmal. The training and testing accuracies started off promising - they were 76% and 79% respectively at epoch 1, but quickly took a rough turn. Both train and test accuracies stayed at exactly 10% for the rest of the entire model, and their average losses were NaN after that epoch.  \n",
        "\n",
        "I attribute this bizarre behavior to the fact that there were no activation functions were implemented in any parts of the model.\n",
        "\n",
        "This model took 11m 3s to execute.\n",
        "_______________________________________________________________________________\n",
        "**Task 2:**\n",
        "\n",
        "This model gave me a final test accuracy of 95.57% and a final training accuracy of 96% at epoch 20.  Both the training and test accuracy reached their peaks at around epoch 2 and stagnated until epoch 20.  The average losses of the two did change a bit from the starting epoch to the final one (a difference of around 1 for the train set and of 0.4 for the test set)\n",
        "\n",
        "\n",
        "This model took 11m 2s to execute.\n",
        "_______________________________________________________________________________\n",
        "**Task 3:**\n",
        "\n",
        "With a final testing accuracy of 97.65%, this was by far my most successful model.  Accuracy for both the training and testing sets reach the high 90's, percentage wise early on (at around the fourth epoch).  The training set finished with an accuracy of 99% at epoch 20.  The average losses for both the training and testing surprisingly did not change too much from the first epoch to the last.\n",
        "\n",
        "This model took 14m 10s to execute.\n",
        "\n",
        "Something interesting to note, is that I tried the ( 2/3* input) + output advice that is suggested by some --vwhen choosing the layers input and neurons. This resulted in slightly better accuracy (around 98.2% accuracy), but a performance of 24m.  A whole 10m more than the model I ended up choosing (for this reason alone)."
      ],
      "metadata": {
        "id": "lKru5lPxGsLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2"
      ],
      "metadata": {
        "id": "uTfjallLi1sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, mode):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        # Define various layers here, such as in the tutorial example\n",
        "        #model 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3)\n",
        "\n",
        "        #models 2 and 3\n",
        "        self.m_conv1 = nn.Conv2d(in_channels=3, out_channels=20, kernel_size=3)\n",
        "        self.m_conv2 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3)\n",
        "        self.m_conv3 = nn.Conv2d(in_channels=40, out_channels=40, kernel_size=3)\n",
        "\n",
        "        \n",
        "        self.fc1_model1 = nn.Linear(360, 100)  # This is first fully connected layer for step 1.\n",
        "        self.fc1_model2 = nn.Linear(1440, 100) # This is first fully connected layer for step 2.\n",
        "        self.fc1_model3 = nn.Linear(640, 100)  # This is first fully connected layer for step 3\n",
        "        \n",
        "        self.fc2 = nn.Linear(100, 10)       # This is 2nd fully connected layer for all models.\n",
        "        \n",
        "        self.fc_model0 = nn.Linear(2250, 100)   # This is for example model.\n",
        "        \n",
        "        \n",
        "        # This will select the forward pass function based on mode for the ConvNet.\n",
        "        # Based on the question, you have 3 modes available for step 1 to 3.\n",
        "        # During creation of each ConvNet model, you will assign one of the valid mode.\n",
        "        # This will fix the forward function (and the network graph) for the entire training/testing\n",
        "        if mode == 1:\n",
        "            self.forward = self.model_1\n",
        "        elif mode == 2:\n",
        "            self.forward = self.model_2\n",
        "        elif mode == 3:\n",
        "            self.forward = self.model_3\n",
        "        elif mode == 0:\n",
        "            self.forward = self.model_0\n",
        "        else: \n",
        "            print(\"Invalid mode \", mode, \"selected. Select between 1-3\")\n",
        "            exit(0)\n",
        "        \n",
        "    \n",
        "    # Example model. Modify this for step 1-3\n",
        "    def model_0(self, X):\n",
        "        # ======================================================================         \n",
        "        \n",
        "        X = F.relu(self.conv1(X))\n",
        "        #print(X.shape)\n",
        "        X = F.max_pool2d(X, kernel_size=2)\n",
        "        #print(X.shape)\n",
        "        \n",
        "        X = torch.flatten(X, start_dim=1)\n",
        "        #print(X.shape)\n",
        "        \n",
        "        X = F.relu(self.fc_model0(X))\n",
        "        X = self.fc2(X)\n",
        "        \n",
        "        return X\n",
        "        \n",
        "    \n",
        "    # Simple CNN. step 1\n",
        "    def model_1(self, X):\n",
        "        # ======================================================================\n",
        "         \n",
        "        # Complete this part as model_0, add one more conv2d layer \n",
        "        # with relu activation followed by maxpool layer.\n",
        "\n",
        "        X = F.relu(self.conv1(X))\n",
        "        #print(X.shape)\n",
        "        X = F.max_pool2d(X, kernel_size=2)\n",
        "        #print(X.shape)\n",
        "        X = F.relu(self.conv2(X))\n",
        "        #print(X.shape)\n",
        "        X = F.max_pool2d(X, kernel_size=2)\n",
        "        \n",
        "        X = torch.flatten(X, start_dim=1)\n",
        "        #print(X.shape)\n",
        "        \n",
        "        X = F.relu(self.fc1_model1(X))\n",
        "        X = self.fc2(X)\n",
        "        \n",
        "        return X\n",
        "        \n",
        "\n",
        "    # Increase filters. step 2\n",
        "    def model_2(self, X):\n",
        "        # ======================================================================\n",
        "        \n",
        "        # Complete this part as model_1. Modify in/out channels for conv2d layers.\n",
        "        \n",
        "        X = F.relu(self.m_conv1(X))\n",
        "        #print(X.shape)\n",
        "        X = F.max_pool2d(X, kernel_size=2)\n",
        "        #print(X.shape)\n",
        "        X = F.relu(self.m_conv2(X))\n",
        "        #print(X.shape)\n",
        "        X = F.max_pool2d(X, kernel_size=2)\n",
        "        \n",
        "        X = torch.flatten(X, start_dim=1)\n",
        "        #print(X.shape)\n",
        "        \n",
        "        X = F.relu(self.fc1_model2(X))\n",
        "        X = self.fc2(X)\n",
        "        \n",
        "        return X\n",
        "        \n",
        "\n",
        "    # Large CNN. step 3\n",
        "    def model_3(self, X):\n",
        "        # ======================================================================\n",
        "        \n",
        "        # Complete this part as model_2, add one more conv2d layer \n",
        "        # with relu activation. Do not add maxpool after this new conv2d layer.\n",
        "        \n",
        "        X = F.relu(self.m_conv1(X))\n",
        "        #print(X.shape)\n",
        "        X = F.max_pool2d(X, kernel_size=2)\n",
        "        #print(X.shape)\n",
        "        X = F.relu(self.m_conv2(X))\n",
        "        #print(X.shape)\n",
        "        X = F.max_pool2d(X, kernel_size=2)\n",
        "        X = F.relu(self.m_conv3(X))\n",
        "        \n",
        "        X = torch.flatten(X, start_dim=1)\n",
        "        #print(X.shape)\n",
        "        \n",
        "        X = F.relu(self.fc1_model3(X))\n",
        "        X = self.fc2(X)\n",
        "        \n",
        "        return X\n",
        "\n",
        "\n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "id": "42y_9riAi5X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import argparse\n",
        "import numpy as np \n",
        "\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch, batch_size):\n",
        "    '''\n",
        "    Trains the model for an epoch and optimizes it.\n",
        "    model: The model to train. Should already be in correct device.\n",
        "    device: 'cuda' or 'cpu'.\n",
        "    train_loader: dataloader for training samples.\n",
        "    optimizer: optimizer to use for model parameter updates.\n",
        "    criterion: used to compute loss for prediction and target \n",
        "    epoch: Current epoch to train for.\n",
        "    batch_size: Batch size to be used.\n",
        "    '''\n",
        "    \n",
        "    # Set model to train mode before each epoch\n",
        "    model.train()\n",
        "    \n",
        "    # Empty list to store losses \n",
        "    losses = []\n",
        "    correct = 0\n",
        "    \n",
        "    # Iterate over entire training samples (1 epoch)\n",
        "    for batch_idx, batch_sample in enumerate(train_loader):\n",
        "        data, target = batch_sample\n",
        "        \n",
        "        # Push data/label to correct device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # print(data.shape)\n",
        "        # print(target.shape)\n",
        "        # exit()\n",
        "        \n",
        "        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Do forward pass for current set of data\n",
        "        output = model(data)\n",
        "        \n",
        "        # ======================================================================\n",
        "        # Compute loss based on criterion\n",
        "        loss = criterion(output,target)\n",
        "        \n",
        "        # Computes gradient based on final loss\n",
        "        loss.backward()\n",
        "        \n",
        "        # Store loss\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        # Optimize model parameters based on learning rate and gradient \n",
        "        optimizer.step()\n",
        "        \n",
        "        # Get predicted index by selecting maximum log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        \n",
        "        # ======================================================================\n",
        "        # Count correct predictions overall \n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        \n",
        "    train_loss = float(np.mean(losses))\n",
        "    train_acc = correct / ((batch_idx+1) * batch_size)\n",
        "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n",
        "        100. * correct / ((batch_idx+1) * batch_size)))\n",
        "    return train_loss, train_acc\n",
        "    \n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    '''\n",
        "    Tests the model.\n",
        "    model: The model to train. Should already be in correct device.\n",
        "    device: 'cuda' or 'cpu'.\n",
        "    test_loader: dataloader for test samples.\n",
        "    '''\n",
        "    \n",
        "    # Set model to eval mode to notify all layers.\n",
        "    model.eval()\n",
        "    \n",
        "    losses = []\n",
        "    correct = 0\n",
        "    \n",
        "    # Set torch.no_grad() to disable gradient computation and backpropagation\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, sample in enumerate(test_loader):\n",
        "            data, target = sample\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "\n",
        "            # Predict for data by doing forward pass\n",
        "            output = model(data)\n",
        "            \n",
        "            # ======================================================================\n",
        "            # Compute loss based on same criterion as training\n",
        "            criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "            loss = criterion(output,target)\n",
        "            \n",
        "            # Append loss to overall test loss\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            # Get predicted index by selecting maximum log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            \n",
        "            # ======================================================================\n",
        "            # Count correct predictions overall \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss = float(np.mean(losses))\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "    \n",
        "    return test_loss, accuracy\n",
        "    \n",
        "\n",
        "def run_main(FLAGS):\n",
        "    # Check if cuda is available\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    \n",
        "    # Set proper device based on cuda availability \n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print(\"Torch device selected: \", device)\n",
        "    \n",
        "    # Initialize the model and send to device \n",
        "    model = ConvNet(FLAGS.mode).to(device)\n",
        "    # print(model)\n",
        "    # exit()\n",
        "\n",
        "    # Initialize the criterion for loss computation \n",
        "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "    \n",
        "    # Initialize optimizer type \n",
        "    optimizer = optim.SGD(model.parameters(), lr=FLAGS.learning_rate, weight_decay=1e-7)\n",
        "    \n",
        "    # Create transformations to apply to each data sample \n",
        "    # Can specify variations such as image flip, color flip, random crop, ...\n",
        "    #transform=transforms.Compose([\n",
        "    #    transforms.ToTensor(),\n",
        "    #    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    #    ])\n",
        "    \n",
        "    transform = transforms.Compose(\n",
        "                    [transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "     \n",
        "    # Load datasets for training and testing\n",
        "    # Inbuilt datasets available in torchvision (check documentation online)\n",
        "    dataset1 = datasets.CIFAR10('./data/', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset2 = datasets.CIFAR10('./data/', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = DataLoader(dataset1, batch_size = FLAGS.batch_size, \n",
        "                                shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(dataset2, batch_size = FLAGS.batch_size, \n",
        "                                shuffle=False, num_workers=4)\n",
        "    \n",
        "    best_accuracy = 0.0\n",
        "    \n",
        "    # Run training for n_epochs specified in config \n",
        "    for epoch in range(1, FLAGS.num_epochs + 1):\n",
        "        print(\"\\nEpoch: \", epoch)\n",
        "        train_loss, train_accuracy = train(model, device, train_loader,\n",
        "                                            optimizer, criterion, epoch, FLAGS.batch_size)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        \n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            \n",
        "    print(\"accuracy is {:2.2f}\".format(best_accuracy))\n",
        "    \n",
        "    print(\"Training and evaluation finished\")\n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    # Set parameters for Sparse Autoencoder\n",
        "    parser = argparse.ArgumentParser('CNN Exercise.')\n",
        "    parser.add_argument('--mode',\n",
        "                        type=int, default=3,\n",
        "                        help='Select mode between 1-3.')\n",
        "    parser.add_argument('--learning_rate',\n",
        "                        type=float, default=0.1,\n",
        "                        help='Initial learning rate.')\n",
        "    parser.add_argument('--num_epochs',\n",
        "                        type=int,\n",
        "                        default=10,\n",
        "                        help='Number of epochs to run trainer.')\n",
        "    parser.add_argument('--batch_size',\n",
        "                        type=int, default=100,\n",
        "                        help='Batch size. Must divide evenly into the dataset sizes.')\n",
        "    parser.add_argument('--log_dir',\n",
        "                        type=str,\n",
        "                        default='logs',\n",
        "                        help='Directory to put logging.')\n",
        "                        \n",
        "    FLAGS = None\n",
        "    FLAGS, unparsed = parser.parse_known_args()\n",
        "    \n",
        "    print(\"Mode: \", FLAGS.mode)\n",
        "    print(\"LR: \", FLAGS.learning_rate)\n",
        "    print(\"Batch size: \", FLAGS.batch_size)\n",
        "    \n",
        "    run_main(FLAGS)\n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "odMueG6wjJXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78103aa7-7f75-4325-b8a6-24564091f940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode:  3\n",
            "LR:  0.1\n",
            "Batch size:  100\n",
            "Torch device selected:  cpu\n",
            "Files already downloaded and verified\n",
            "\n",
            "Epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: Average loss: 1.9504, Accuracy: 13921/50000 (28%)\n",
            "Test set: Average loss: 1.6096, Accuracy: 4172/10000 (42%)\n",
            "\n",
            "Epoch:  2\n",
            "Train set: Average loss: 1.5252, Accuracy: 22131/50000 (44%)\n",
            "Test set: Average loss: 1.4445, Accuracy: 4778/10000 (48%)\n",
            "\n",
            "Epoch:  3\n",
            "Train set: Average loss: 1.3432, Accuracy: 25942/50000 (52%)\n",
            "Test set: Average loss: 1.2614, Accuracy: 5487/10000 (55%)\n",
            "\n",
            "Epoch:  4\n",
            "Train set: Average loss: 1.2000, Accuracy: 28805/50000 (58%)\n",
            "Test set: Average loss: 1.1634, Accuracy: 5863/10000 (59%)\n",
            "\n",
            "Epoch:  5\n",
            "Train set: Average loss: 1.0897, Accuracy: 30834/50000 (62%)\n",
            "Test set: Average loss: 1.0810, Accuracy: 6184/10000 (62%)\n",
            "\n",
            "Epoch:  6\n",
            "Train set: Average loss: 0.9941, Accuracy: 32544/50000 (65%)\n",
            "Test set: Average loss: 1.0995, Accuracy: 6090/10000 (61%)\n",
            "\n",
            "Epoch:  7\n",
            "Train set: Average loss: 0.9273, Accuracy: 33764/50000 (68%)\n",
            "Test set: Average loss: 0.9815, Accuracy: 6512/10000 (65%)\n",
            "\n",
            "Epoch:  8\n",
            "Train set: Average loss: 0.8516, Accuracy: 35100/50000 (70%)\n",
            "Test set: Average loss: 0.9131, Accuracy: 6745/10000 (67%)\n",
            "\n",
            "Epoch:  9\n",
            "Train set: Average loss: 0.7973, Accuracy: 35976/50000 (72%)\n",
            "Test set: Average loss: 0.8967, Accuracy: 6844/10000 (68%)\n",
            "\n",
            "Epoch:  10\n",
            "Train set: Average loss: 0.7446, Accuracy: 36983/50000 (74%)\n",
            "Test set: Average loss: 0.9165, Accuracy: 6813/10000 (68%)\n",
            "accuracy is 68.44\n",
            "Training and evaluation finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:**\n",
        "\n",
        "The final training accuracy was 69% epoch 10, while the test accuracy was 63.65%.  The average losses dropped for both sets, but it seems that the test's average loss did not improve too much, as it only dropped around 0.4 between epoch 1 and 10.  Meanwhile, the average loss difference for the training set from epoch 1 and 10 was around 1.\n",
        "\n",
        "This model took 4m 37s to execute.\n",
        "_______________________________________________________________________________\n",
        "**Task 5:**\n",
        "\n",
        "This model gave me a final test accuracy of 69.19% and a final training accuracy of 80% at epoch 10.  While the training accuracy kept improving as the model went along, the testing accuracy sort of remained stagnant after epoch 6/7.\n",
        "\n",
        "The average losses for both the train and test sets did linearly improve though, as epochs passed.\n",
        "\n",
        "This model took 8m 15s to execute.\n",
        "_______________________________________________________________________________\n",
        "**Task 6:**\n",
        "\n",
        "The final testing acurracy here was 68.44%.  The training set finished with an accuracy of 74% at epoch 10.  The average losses for both the training and testing sets dropped as new epochs ran.  The only peculiar behavior I can report is the testing accuracy also stagnated at a certain point, though only at epoch 9.  \n",
        "\n",
        "As a sidenote, I was surprised that this models accuracy was very similar to the previous model.  I assumed that with an extra convolution layer, the performance would only be better.\n",
        "\n",
        "This model took 8m 48s to execute."
      ],
      "metadata": {
        "id": "QPiCxvg-6l2e"
      }
    }
  ]
}